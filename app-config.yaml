app:
  title: Scaffolded Backstage App
  baseUrl: http://localhost:3000

backend:
  baseUrl: http://localhost:7007
  listen:
    port: 7007
  csp:
    connect-src: ["'self'", 'http:', 'https:']
  cors:
    origin: http://localhost:3000
    methods: [GET, HEAD, PATCH, POST, PUT, DELETE]
    credentials: true
  database:
    client: pg
    connection:
      host: localhost
      port: 5432
      user: postgres
      password: postgres

auth:
  providers:
    guest:
      userEntityRef: user:default/bob
      ownershipEntityRefs: [group:default/devops]
      dangerouslyAllowOutsideDevelopment: false

# see https://backstage.io/docs/permissions/getting-started for more on the permission framework
permission:
  # setting this to `false` will disable permissions
  enabled: true

catalog:
  orphanStrategy: delete
  processingInterval: { minutes: 1 }
  rules:
    - allow:
        - User
        - Group
        - System
        - Component
        - Domain
        - API
        - Resource
  locations:
    - type: file
      target: ../../catalog.local-dev.yaml

aiAssistant:
  prompt:
    system: |
      You are a helpful AI assistant called freddy that helps users with their software development tasks.s
      When you are given a question, you should first try to answer it using your own knowledge. If you are unable to answer the question, you should use one of the tools at your disposal to find the answer. Once you have found the answer, you should provide it to the user in a clear and concise manner.
  conversation:
    summaryModel: <your-summary-model> # Must be in your list of models

  storage:
    pgVector:
      chunkSize: 800
      amount: 10
  ingestion:
    schedule:
      frequency:
        days: 1
      timeout:
        hours: 1
  embeddings:
    ollama:
      model: '<your-model>'
      apiKey: '<your-api-key>'
      baseUrl: '<your-base-url>'
    azureOpenAi:
      deploymentName: '<your-deployment-name>'
      instanceName: '<your-instance-name>'
      apiKey: '<your-api-key>'
      endpoint: '<your-azure-openai-endpoint>'
      openAIApiVersion: <your-deployments-openai-version>

  models:
    ollama:
      baseUrl: '<your-base-url>'
      apiKey: '<your-api-key>'
      models:
        - '<your-model>'
    azureAi:
      apiKey: <your-api-key>
      models:
        - endpoint: <your-azure-ai-foundry-endpoint>
          modelName: '<your-model>'

ingestors:
  # See ./plugins/ai-assistant-backend-module-ingestor-github/config.d.ts for more config options
  github:
    owner: <your-github-org-or-username>
    appId: <your-github-app-id>
    privateKey: <your-github-private-key>
    installationId: <your-github-installation-id>

    # optional config to filter what to ingest
    # filesBatchSize: <number-of-files-to-process-in-batches> # default: 50
    # baseUrl: <your-github-enterprise-instance-url> # only needed for GitHub Enterprise
    # fileTypes:
    #   - '.md'
    # repositories:
    #   - name: <repository-name>
    #     fileTypes:
    #       - '.json'

  # See ./plugins/ai-assistant-backend-module-ingestor-azure-devops/config.d.ts for more config options
  azureDevOps:
    organization: <your-azure-devops-org-name>
    project: <your-project-name>
    token: <your-azure-ado-token>
    resourceTypes:
      # specify one or both of the following resource types to ingest
      - 'repository'
      - 'wiki'

    # optional config to filter what to ingest
    # fileTypes:
    #   - '.md'
    #   - '.json'
    #   - '.txt'
    # repositories:
    #   - name: <repository-name>
    #     fileTypes:
    #       - '.json'
    #       - '.md'
    #       - '.txt'
    # filesBatchSize: <number-of-repository-files-to-process-in-batches> # default: 50

    # wikis:
    #   - name: <wiki-name>
    # pagesBatchSize: <number-of-wiki-pages-to-process-in-batches> # default: 50
